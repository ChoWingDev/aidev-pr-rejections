%%
%% This is file `sample-manuscript.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,manuscript')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[acmsmall,screen,review]{acmart}
\usepackage{graphicx}
\usepackage{float}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmlicensed}
% \copyrightyear{2018}
% \acmYear{2018}
% \acmDOI{XXXXXXX.XXXXXXX}
% %% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Conference acronym 'XX]{Make sure to enter the correct
%   conference title from your rights confirmation email}{June 03--05,
%   2018}{Woodstock, NY}
% %%
% %%  Uncomment \acmBooktitle if the title of the proceedings is different
% %%  from ``Proceedings of ...''!
% %%
% %%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
% %%  June 03--05, 2018, Woodstock, NY}
% \acmISBN{978-1-4503-XXXX-X/2018/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Unpacking Rejections in AI-Generated Pull Requests}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Hwimin Park}
\author{Cho Wing Chan}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
% \renewcommand{\shortauthors}{Trovato et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
% \begin{abstract}
%     (placeholder)
% \end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%


%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
The integration of AI in SE practices has been further accelerated dramatically, turning traditional ways of development into collaborative ecosystems involving human developers and autonomous AI coding agents. These agents, such as Devin, GitHub Copilot, and OpenAI Codex, now power real-world projects by writing code, solving issues, and creating pull requests on platforms such as GitHub. However, comprehending their impact requires empirical analysis of large-scale in-the-wild data to uncover patterns in agent behavior, collaboration dynamics, and project outcomes. The AIDev dataset addresses this need by providing a comprehensive collection of more than 456,000 PRs authored by five key AI agents across various open-source repositories \cite{li2025aidev}. It includes features such as agent type, PR acceptance rates, issue complexity scores, commit frequencies, and repository growth measurements, enabling nuanced explorations of AI’s role in the era of AI teammates.

This project employs the AIDev dataset to investigate three research questions (RQs) that explore multiple feature interactions within the dataset to identify actionable insights for improving AI–human collaboration in SE.\\
\textbf{RQ1:} What common failure patterns cause AI-generated PRs to be rejected?\\
\textbf{RQ2:} How consistent are AI-generated PR descriptions with the actual code changes?\\
\textbf{RQ3:} Do AI-generated PRs attract different types or amounts of reviewer comments compared to human PRs?\\
Our team consists of two members: Hwimin Park, who led data preprocessing and RQ1 implementation; and Cho Wing Chan, who handled RQ2 analysis, RQ3 methodology planning, and repository management. GenAI (specifically, Windsurf \cite{windsurf2025}) was used for autocompletion during code writing, which was manually reviewed and verified for correctness before integration. This helped speed up initial drafting but did not generate core analysis logic.

We present here the progress on half the project in this Milestone 2 report: full methodologies, implementations, and interpretations for RQ1 and RQ2, with preliminary results highlighting key trends. RQ3 remains at the planning stage, with analysis slated for completion in Milestone 3. The report follows with Section 2 detailing methodologies, Section 3 discussing results and interpretations, and Section 4 concluding with next steps. Code and data are available at our GitHub repository: \url{https://github.com/ChoWingDev/aidev-pr-rejections}.

\section{Methodology}
\subsection{RQ1 Methodology}
The following is an examination of the failure patterns that cause AI-generated pull requests to be rejected. We analyzed a subset of PRs in the AIDev dataset that were marked both as AI-generated and not merged. This filtering produced 9,582 rejected AI PRs for analysis. We first examined reviewer feedback by extracting all related comments and categorizing them into five groups: correctness, tests, style, security, and other, using a rule-based keyword approach. This allowed us to quantify the types of reviewer concerns most frequently raised during rejection. Next, we computed patch-level characteristics for each PR, based on the project's simplified feature plan. These included the number of files changed, the magnitude of changes (measured through additions, deletions, and total changes), and the distribution of changed file types across test, documentation, and source directories. This step gave a structural overview of AI-generated patches, allowing us to analyze whether certain patch shapes correlated with rejection reasons.

Finally, to uncover deeper rejection patterns, we applied K-Means clustering (k = 3) over the combined patch features and normalized comment-category signals. This unsupervised method allowed us to group rejected AI PRs into coherent behavioral clusters, revealing recurring failure modes such as extremely large source-code changes, documentation-dominated patches, or test-focused modifications that still triggered reviewer dissatisfaction. These clusters are summarized via a radar chart in Figure~\ref{fig:clusters}. Together, these steps form a comprehensive methodology for identifying and characterizing the reasons AI PRs fail during human review.

\begin{figure}[H]
\centering
\includegraphics[width=0.6\columnwidth]{fig_rq1_clusters.png}
\caption{Radar chart of K-Means clusters (k=3) for rejected AI PRs, normalized across features. Cluster 1: Documentation-heavy; Cluster 2: Test-focused but critiqued; Cluster 3: Large source changes with correctness issues.}
\label{fig:clusters}
\end{figure}

\subsection{RQ2 Methodology}
To measure the consistency with which AI-generated pull request descriptions accurately describe actual code changes, we performed a systematic two-stream analysis directly comparing claimed intent in PR text against concrete file-level modifications in the AIDev dataset. From the `pull\_request` table, we extracted intent signals by scanning titles and bodies for high-precision keywords: “fix” (bug fixing), “test” or “add tests” (test-related changes), “refactor” (code reorganization), and “doc” or “documentation” (documentation updates). At the same time, from the `pr\_commit\_details` table, we extracted ground-truth behavioral signals, including the types of files modified (source, test, documentation, or unknown), total added and deleted lines, sum change magnitude, and secondary intent clues within patch text itself.  

Using these parallel representations, we defined four mutually exclusive consistency categories via deterministic rules: aligned (description matches file-level behavior), missing-tests (claims test additions but no test files touched), fix-doc-mismatch (claims bug fix but modifies only documentation), and refactor-mismatch (claims refactoring but exhibits large net additions typical of new features). This rule-based classification facilitated the quantitative assessment of description fidelity across tens of thousands of AI-authored PRs without requiring expensive LLM judgments.

\section{Results and Interpretation}
\subsection{RQ1 Results and Interpretation}
Analysis of the comments left by reviewers revealed that the most frequent reason for refusing AI-sourced PRs was test-related, with 1,393 complaints, followed by 1,094 correctness-related issues. Style issues appeared less frequently, and explicit security concerns were relatively rare. Although a large number of comments were categorized as “other,” manual inspection showed that many of these were non-substantive, such as conversational remarks or generic automation messages. This left correctness and testing as the primary meaningful rejection drivers.

Patch-level statistics reinforced these trends. The median percentage of test files modified was zero, indicating that most AI-generated patches resulted in no changes to testing. Meanwhile, numerous PRs had significant code churn, with an average of more than 2,000 added lines and over 1,000 deletions. This combination, large code changes with no accompanying tests, suggests structural shortcomings in how AI systems create and verify patches.

Clustering analysis revealed three primary failure patterns. One cluster consisted of documentation-heavy PRs with little or no test coverage, often receiving style-oriented reviewer feedback or comments questioning their relevance. A second cluster contained PRs that modified a relatively high proportion of test files but still drew test-related critiques, suggesting that AI-generated tests often fail to meet reviewer expectations. The third cluster captured large source-code patches with the highest volume of modifications and the strongest concentration of correctness complaints, reflecting cases where AI attempted wide-ranging edits without maintaining logical consistency.

Taken together, these findings suggest that AI-generated PRs are rejected primarily due to misalignment between claimed intentions and actual changes, missing or low-quality tests, or overly broad and incorrect code modifications. These patterns highlight critical weaknesses in current AI code-generation workflows and point toward better testing enforcement, improved patch scoping, and stronger alignment between PR descriptions and code content as key areas for improvement.

\subsection{RQ2 Results and Interpretation}
The results show that AI-generated PR descriptions accurately reflect the actual code changes in only about half of the cases, specifically, 16,812 aligned PRs, with the rest systematically mismatched. The most common inconsistency, missing-tests, appeared in 12,676 PRs (37.7\%), where AI claimed to add or improve tests yet modified no test files, often altering only source code. The fix-doc-mismatch affected 2,666 PRs (7.9\%), where descriptions labeled changes as bug fixes even though only documentation files were modified. Refactor-mismatch occurred in 1,426 cases (4.2\%), where claimed refactorings instead introduced large volumes of new code rather than balanced edits.

Boxplot analysis of the magnitude of change confirmed these behavioral patterns. Documentation-only PRs had the fewest changes (median less than 50 lines), while refactor-mismatch cases displayed distributions similar to typical feature additions. File-type analysis further validated these findings: aligned PRs modified a balanced mix of source (approximately 14,000), test (approximately 14,000), and documentation (approximately 7,000) files, whereas missing-tests PRs showed almost no involvement with test files despite test-related claims.

Overall, these results indicate that current AI agents often overstate or misrepresent the nature of their contributions. Test-related claims are the least reliable, documentation edits are frequently mislabeled as bug fixes, and refactoring intents are commonly confused with feature development. These patterns emphasize that AI-generated PR descriptions should be treated as suggestive rather than authoritative, reinforcing the need for human review and automated diff-validation tools in AI-assisted software workflows.


\section{Conclusion and Next Steps}
This Milestone 2 report shows that AI-generated PRs have two important weaknesses: their changes are often rejected because of missing or insufficient tests and overly broad correctness failures (RQ1); and their descriptive texts properly reflect actual code changes in only $\sim$50\% of cases, with test-related claims being the most exaggerated (RQ2). These regular patterns reveal that current AI coding agents still lack reliable test generation, change scoping, and truthful self-description, making human review indispensable. For Milestone 3, we will finish RQ3 by comparing volume, tone, and categories of reviewer comments between AI and matched human PRs, integrate all findings, add the planned figures for RQ1 and RQ2 that are omitted here due to the 4-page limit, and extend the report to 5–7 pages with broader discussion and recommendations. The final polished code and results will remain available at \url{https://github.com/ChoWingDev/aidev-pr-rejections}.


\bibliographystyle{ACM-Reference-Format}

\begin{thebibliography}{2}

\bibitem{li2025aidev}
Hao~Li, et~al.
\newblock The rise of ai teammates in software engineering (se) 3.0.
\newblock {\em arXiv preprint arXiv:2507.15003}, 2025.

\bibitem{windsurf2025}
Windsurf Team.
\newblock Windsurf: The best AI for coding.
\newblock {\em Online service}, 2025.
\url{https://windsurf.com}.

\end{thebibliography}

% @article{li2025aidev,
% title={{The Rise of AI Teammates in Software Engineering (SE) 3.0: How Autonomous Coding Agents Are Reshaping Software Engineering}}, 
% author={Li, Hao and Zhang, Haoxiang and Hassan, Ahmed E.},
% journal={arXiv preprint arXiv:2507.15003},
% year={2025}
% }




% \section{Dataset}

% The AIDev dataset is an open collection of AI-authored pull requests from GitHub, capturing AI-human software collaborations. It features a curated subset of pull requests from popular projects, enriched with code changes, comments, reviews, commits, and issues from tools like GitHub Copilot, Devin, and Claude Code, supporting research on AI adoption, patch quality, and dynamics.

% \section{Research Questions}

% \begin{enumerate}
%     \item \textbf{What common failure patterns cause AI-generated PRs to be rejected?}\\
%     This research question identifies prevalent failure patterns, such as inaccuracies, insufficient testing, stylistic issues, and security risks, leading to rejection of AI-generated pull requests. To address it, select rejected AI PRs from the dataset. Then categorize reviewer comments into domains like correctness, tests, style, and security. Next, record patch metrics including size, entropy, and path depth. Conduct frequency analysis to pinpoint dominant issues, then apply clustering or association rules to reveal patterns, such as large untested patches or security folder modifications, which commonly drive rejections.
    
%     \item \textbf{How consistent are AI-generated PR descriptions with the actual code changes?}\\
%     This question evaluates alignment between AI-generated PR descriptions and code modifications, highlighting discrepancies that erode trust. Steps needed include: extracting keywords from descriptions (such as "fix bug," "refactor"), parsing code diffs for operations like file additions, deletions, or test/source distinctions, and lastly compute similarity via a Keyword-Diff Alignment Score. Identifying mismatches, such as bug-fix claims altering only documentation, and validating via reviewer comments on inaccuracies will also need to be done. These findings will inform strategies for improving AI prompt engineering to enhance descriptive accuracy.
    
%     \item \textbf{What early signals predict whether an AI-generated PR will be accepted or rejected?}\\
%     This research question uncovers submission-time indicators forecasting AI-generated PR outcomes before reviewer input. Methods include compiling initial features, building models like logistic regression, evaluating with metrics, and using SHAP values to rank signals as key predictors. Such insights can guide automated checks or refinements in AI generation pipelines to boost acceptance rates.
% \end{enumerate}


% \section{Dataset and Wrangling Pipeline}
% As noted in the introduction, the ``\verb|acmart|'' document class can
% be used to prepare many different kinds of documentation --- a
% double-anonymous initial submission of a full-length technical paper, a
% two-page SIGGRAPH Emerging Technologies abstract, a ``camera-ready''
% journal article, a SIGCHI Extended Abstract, and more --- all by
% selecting the appropriate {\itshape template style} and {\itshape
%   template parameters}.

% This document will explain the major features of the document
% class. For further information, the {\itshape \LaTeX\ User's Guide} is
% available from
% \url{https://www.acm.org/publications/proceedings-template}.

% \subsection{Template Styles}

% The primary parameter given to the ``\verb|acmart|'' document class is
% the {\itshape template style} which corresponds to the kind of publication
% or SIG publishing the work. This parameter is enclosed in square
% brackets and is a part of the {\verb|documentclass|} command:
% \begin{verbatim}
%   \documentclass[STYLE]{acmart}
% \end{verbatim}

% Journals use one of three template styles. All but three ACM journals
% use the {\verb|acmsmall|} template style:
% \begin{itemize}
% \item {\texttt{acmsmall}}: The default journal template style.
% \item {\texttt{acmlarge}}: Used by JOCCH and TAP.
% \item {\texttt{acmtog}}: Used by TOG.
% \end{itemize}

% The majority of conference proceedings documentation will use the {\verb|acmconf|} template style.
% \begin{itemize}
% \item {\texttt{sigconf}}: The default proceedings template style.
% \item{\texttt{sigchi}}: Used for SIGCHI conference articles.
% \item{\texttt{sigplan}}: Used for SIGPLAN conference articles.
% \end{itemize}

% \subsection{Template Parameters}

% In addition to specifying the {\itshape template style} to be used in
% formatting your work, there are a number of {\itshape template parameters}
% which modify some part of the applied template style. A complete list
% of these parameters can be found in the {\itshape \LaTeX\ User's Guide.}

% Frequently-used parameters, or combinations of parameters, include:
% \begin{itemize}
% \item {\texttt{anonymous,review}}: Suitable for a ``double-anonymous''
%   conference submission. Anonymizes the work and includes line
%   numbers. Use with the \texttt{\string\acmSubmissionID} command to print the
%   submission's unique ID on each page of the work.
% \item{\texttt{authorversion}}: Produces a version of the work suitable
%   for posting by the author.
% \item{\texttt{screen}}: Produces colored hyperlinks.
% \end{itemize}

% This document uses the following string as the first command in the
% source file:
% \begin{verbatim}
% \documentclass[manuscript,screen,review]{acmart}
% \end{verbatim}

% \section{Methodology and Results}

% Modifying the template --- including but not limited to: adjusting
% margins, typeface sizes, line spacing, paragraph and list definitions,
% and the use of the \verb|\vspace| command to manually adjust the
% vertical spacing between elements of your work --- is not allowed.

% {\bfseries Your document will be returned to you for revision if
%   modifications are discovered.}

% \subsection{Answering RQ1: Balance of Additions vs. Deletions by Agent and Repo Size}

% \section{Discussion}

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
% \bibliographystyle{ACM-Reference-Format}
% \bibliography{sample-base}


%%
%% If your work has an appendix, this is the place to put it.
% \appendix

\end{document}
\endinput
%%
%% End of file `sample-manuscript.tex'.
